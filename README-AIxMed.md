LLaVA-Med v1.5, using mistralai/Mistral-7B-Instruct-v0.2 as LLM for a better commercial license

Large Language and Vision Assistant for bioMedicine (i.e., “LLaVA-Med”) is a large language and vision model trained using a curriculum learning method for adapting LLaVA to the biomedical domain. It is an open-source release intended for research use only to facilitate reproducibility of the corresponding paper which claims improved performance for open-ended biomedical questions answering tasks, including common visual question answering (VQA) benchmark datasets such as PathVQA and VQA-RAD.

Latest model: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3



[CLIPVisionTower] cfg_only = CLIPVisionConfig {
2024-09-30 16:28:09 | INFO | stdout |   "attention_dropout": 0.0,
2024-09-30 16:28:09 | INFO | stdout |   "dropout": 0.0,
2024-09-30 16:28:09 | INFO | stdout |   "hidden_act": "quick_gelu",
2024-09-30 16:28:09 | INFO | stdout |   "hidden_size": 1024,
2024-09-30 16:28:09 | INFO | stdout |   "image_size": 336,
2024-09-30 16:28:09 | INFO | stdout |   "initializer_factor": 1.0,
2024-09-30 16:28:09 | INFO | stdout |   "initializer_range": 0.02,
2024-09-30 16:28:09 | INFO | stdout |   "intermediate_size": 4096,
2024-09-30 16:28:09 | INFO | stdout |   "layer_norm_eps": 1e-05,
2024-09-30 16:28:09 | INFO | stdout |   "model_type": "clip_vision_model",
2024-09-30 16:28:09 | INFO | stdout |   "num_attention_heads": 16,
2024-09-30 16:28:09 | INFO | stdout |   "num_channels": 3,
2024-09-30 16:28:09 | INFO | stdout |   "num_hidden_layers": 24,
2024-09-30 16:28:09 | INFO | stdout |   "patch_size": 14,
2024-09-30 16:28:09 | INFO | stdout |   "projection_dim": 768,
2024-09-30 16:28:09 | INFO | stdout |   "transformers_version": "4.45.1"
2024-09-30 16:28:09 | INFO | stdout | }
